{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data processing\n",
        "\n",
        "\n",
        "\n",
        "This code processes a collection of text files located in a specified directory. It performs the following tasks:<br>\n",
        "\n",
        "\n",
        "**Data Collection and Preprocessing:**<br>\n",
        "\n",
        " - Retrieves a list of text files in a specified directory.\n",
        " - Reads and combines the text from the first file in the list.\n",
        " - Extracts information such as sex labels and year labels from the file paths.\n",
        " - Creates a DataFrame ('ori_df') to store original text, sex labels, and year labels.\n",
        " - Displays the first few rows of the DataFrame.\n",
        "\n",
        "**Text Tokenization and Filtering:**<br>\n",
        "\n",
        " - Loads a French tokenizer and stopwords for text processing.\n",
        " - Tokenizes the text from the first file using the French tokenizer.\n",
        " - Defines a function to segment and remove stopwords from French text.\n",
        " - Processes each document by segmenting and removing stopwords.\n",
        " - Prints the length of the filtered text.\n",
        "\n",
        "**TF-IDF Vectorization:**<br>\n",
        "\n",
        " - Utilizes the TF-IDF vectorizer to convert the processed text into a numerical format.\n",
        " - Prints the shape of the resulting TF-IDF matrix.\n",
        "\n",
        "**Saving Results:**<br>\n",
        "\n",
        " - Saves the TF-IDF matrix as a NumPy array in a file named 'tf-idf.npy'.\n",
        " - Writes sex labels to a text file named 'sex.txt'.\n",
        "\n",
        "In summary, the code performs text data preprocessing, tokenization, TF-IDF vectorization, and saves the processed data for further analysis or machine learning tasks."
      ],
      "metadata": {
        "id": "97XI9B40DjB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0. Importing Libraries"
      ],
      "metadata": {
        "id": "Z7MhOxAyMX_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTiccSBeDhMy"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import warnings\n",
        "import pandas as pd\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "import re, glob\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import jieba\n",
        "from functools import reduce\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. Data Collection and Preprocessing"
      ],
      "metadata": {
        "id": "8HR-NxrFMzGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of text files in the specified directory\n",
        "txt_list = glob.glob(\"./data_text/*/*.txt\")\n",
        "print(len(txt_list))\n",
        "print(txt_list[0])"
      ],
      "metadata": {
        "id": "f9W6VjNuDiuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and join the text from the first file in the list\n",
        "ori_text = [line.strip() for line in open(txt_list[0], encoding='UTF-8').readlines() if line != '\\n']\n",
        "ori_text = \" \".join(ori_text)\n",
        "print(ori_text)"
      ],
      "metadata": {
        "id": "q9rqqc7kDnHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store sex labels, year labels, and processed text\n",
        "sex_list = []\n",
        "year_list = []\n",
        "ori_text_list = []\n",
        "\n",
        "# Iterate through each text file\n",
        "for txt_path in tqdm(txt_list):\n",
        "    try:\n",
        "        # Extract year label from the file path\n",
        "        year_label = int(txt_path.split('(')[5][:4])\n",
        "        year_list.append(year_label)\n",
        "\n",
        "        # Extract sex label from the file path\n",
        "        sex_label = int(txt_path.split('(')[4][:1])\n",
        "        sex_list.append(sex_label)\n",
        "\n",
        "        # Read and join the text from the current file\n",
        "        ori_text = [line.strip() for line in open(txt_path, encoding='UTF-8').readlines() if line != '\\n']\n",
        "        ori_text = \" \".join(ori_text)\n",
        "        ori_text_list.append(ori_text)\n",
        "    except:\n",
        "        print(txt_path)\n",
        "\n",
        "print(len(sex_list), len(year_list), len(ori_text_list))\n"
      ],
      "metadata": {
        "id": "t1wB9yC3DxLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DataFrame and then displaying the first few rows.\n",
        "ori_df = pd.DataFrame({'ori_text':ori_text_list, 'sex':sex_list, 'year':year_list})\n",
        "ori_df.head()"
      ],
      "metadata": {
        "id": "4IjBdSw4Dzls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('sex.txt', 'w') as f:\n",
        "#     for item in sex_list:\n",
        "#         f.write(\"%s\\n\" % item)\n",
        "\n",
        "with open('year.txt', 'w') as f:\n",
        "    for item in year_list:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "GVPAfk122IOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 2. Text Tokenization and Filtering"
      ],
      "metadata": {
        "id": "8x36Y6yGMe1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load French tokenizer and stopwords for further text processing\n",
        "tokenizer_french = nltk.data.load('tokenizers/punkt/french.pickle')\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "# Tokenize the text from the first file using French tokenizer\n",
        "result = word_tokenize(text=ori_text_list[0], language='french')\n",
        "print(result)"
      ],
      "metadata": {
        "id": "uYHeDgr5D0Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for segmenting and removing stopwords from French text\n",
        "def seg_depart(sentence, stopwords):\n",
        "    # Tokenize each line in the document using French tokenizer\n",
        "    result = word_tokenize(text=sentence, language='french')\n",
        "\n",
        "    # Define a regular expression to match pure numbers and pure punctuation\n",
        "    regex = re.compile('^\\d+$|^[^\\w\\s]+$')\n",
        "\n",
        "    # Remove pure numbers and pure punctuation using the regular expression\n",
        "    tokens = [token for token in result if not regex.match(token) and token not in stopwords]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "v62eK1C-D8CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each document by segmenting and removing stopwords\n",
        "filtered_texts = []\n",
        "for ori_text in tqdm(ori_text_list[:]):\n",
        "    filtered_text = seg_depart(ori_text, stop_words)\n",
        "    filtered_texts.append(filtered_text)\n",
        "print(len(filtered_texts))"
      ],
      "metadata": {
        "id": "0nU2CI0ZD847"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 3. TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "I4zj-fkzMocm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TF-IDF vectorizer to convert the processed text into numerical format\n",
        "tf_vectorizer = TfidfVectorizer(max_features=10000) # Use TF-IDF for numerical processing\n",
        "tf_fit = tf_vectorizer.fit_transform(filtered_texts)\n",
        "print(tf_fit.shape)"
      ],
      "metadata": {
        "id": "qJnno9EnEAQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 4. Saving Results"
      ],
      "metadata": {
        "id": "oh13BqzAMrI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the TF-IDF matrix as a NumPy array\n",
        "np.save('tf-idf.npy', tf_fit.toarray())"
      ],
      "metadata": {
        "id": "1fMRfXcZEB_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write sex labels to a text file\n",
        "with open('sex.txt', 'w') as f:\n",
        "    for item in sex_list:\n",
        "        # Write to the file\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "MDCVgVirEGAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}